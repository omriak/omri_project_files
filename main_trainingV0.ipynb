{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOnkBHbx1dsBljSUwMYZW1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omriak/omri_project_files/blob/main/main_trainingV0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spITVIsIY05j",
        "outputId": "3777ac74-37a3-4b7a-c584-e14c23d9298d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports section & globals parameters"
      ],
      "metadata": {
        "id": "88xDES6LZHp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Resizing\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# main path:\n",
        "path_folder = os.path.join('/content/drive/MyDrive/Project_OmriAkrish/project_code_files/training_data')\n",
        "\n"
      ],
      "metadata": {
        "id": "7JsavNyoY-Yn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function section"
      ],
      "metadata": {
        "id": "UI-og4eCZMjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upsample_100(data):\n",
        "    current_N = np.shape(data)\n",
        "    new_data = np.zeros(shape=(100,current_N[1],current_N[2]))\n",
        "    for i in range(0,100):\n",
        "        new_data[i,:,:] = data[int(np.floor((current_N[0]*i)/100)),:,:]\n",
        "    return new_data\n",
        "\n",
        "def DFS():\n",
        "    DFS_indexlist = np.array([2, 21, 9, 10,  11,  12, 25,  12,  24,  12,  11, 10, 9,\n",
        "                             21, 3, 4, 3, 21, 5, 6, 7, 8, 23, 8, 22, 8, 7, 6, 5, 21,\n",
        "                             2, 1, 13, 14, 15, 16, 15, 14, 13, 1, 17, 18, 19, 20, 19,\n",
        "                             18, 17, 1, 2])\n",
        "    DFS_indexlist = DFS_indexlist-1\n",
        "    return DFS_indexlist\n",
        "\n",
        "# Nicely formatted time string\n",
        "\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "def build_image_resizer(Oheight,Owidth,Iheight,Iwidth,R):\n",
        "    model = Sequential()\n",
        "    model.add(Resizing(Oheight, Owidth, interpolation=\"bilinear\" , input_shape=(Iheight,Iwidth,R)))\n",
        "    return model\n",
        "\n",
        "def build_generator(seed_size):\n",
        "    # seed_size is z+c where c is the class sagment\n",
        "    model = Sequential()\n",
        "    \n",
        "    #input layer\n",
        "    model.add(Dense(4*4*512,activation=\"relu\",input_dim=seed_size)) \n",
        "    model.add(Reshape((4,4,512)))\n",
        "\n",
        "    # Mid CNN layer\n",
        "    model.add(UpSampling2D(size=(2, 2),interpolation = 'bilinear'))\n",
        "    model.add(Conv2D(256,kernel_size=(3,3),strides = (1, 1),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    model.add(UpSampling2D(size=(2, 2),interpolation = 'bilinear'))\n",
        "    model.add(Conv2D(128,kernel_size=(3,3),strides = (1, 1),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    model.add(UpSampling2D(size=(2, 2),interpolation = 'bilinear'))\n",
        "    model.add(Conv2D(64,kernel_size=(3,3),strides = (1, 1),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    \n",
        "    # Final CNN layer\n",
        "    model.add(UpSampling2D(size=(2, 2),interpolation = 'bilinear'))\n",
        "    model.add(Conv2D(2,kernel_size=(3,3),strides = (1, 1),padding=\"same\",use_bias = False))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator(image_shape):\n",
        "    model = Sequential()\n",
        "    \n",
        "    #input layer first using resizer\n",
        "    model.add(Conv2D(64, kernel_size=(4,4), strides=(2,2), input_shape=image_shape, padding=\"same\",use_bias = False))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, kernel_size=(4,4), strides=(2,2),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(256, kernel_size=(4,4), strides=(2,2),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(Conv2D(512, kernel_size=(4,4), strides=(2,2),padding=\"same\",use_bias = False))\n",
        "    model.add(BatchNormalization(epsilon=10**-5,momentum=0.1))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "#traning module\n",
        "\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def add_class(images):\n",
        "    images = images.numpy()\n",
        "    s = images.shape\n",
        "    zero = np.zeros((s[0], s[1], s[2], 5))  \n",
        "    one = np.ones((s[0], s[1], s[2], 1)) \n",
        "    class1 = np.append(zero, one, axis=3)\n",
        "    return  np.append(images, class1, axis=3)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images,seed):\n",
        "    generator_resizer = build_image_resizer(128,128)\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: \n",
        "        generated_images = generator(seed, training=True)\n",
        "        tf.concat([generated_images,tf.ones((generated_images.shape[0],64,64,1)),tf.zeros((generated_images.shape[0],64,64,5)), tf.ones((generated_images.shape[0],64,64,1))], 3)\n",
        "        #generated_images = generator_resizer(generated_images, training=False)\n",
        "        #proto_tensor = tf.make_tensor_proto(generated_images)\n",
        "        #generated_images = tf.make_ndarray(proto_tensor)\n",
        "        \n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "    \n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables),\n",
        "    experimental_aggregate_gradients=False)\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables),\n",
        "    experimental_aggregate_gradients=False)\n",
        "    return gen_loss,disc_loss\n",
        "\n",
        "def one_seed():\n",
        "    seed = np.random.normal(0, 1,100)\n",
        "    c = np.asarray((0,0,0,0,0,1))\n",
        "    fixed_seed = np.append(seed,c)\n",
        "    return fixed_seed\n",
        "\n",
        "def batch_seed(BATCH_SIZE):\n",
        "    seed = np.random.normal(0, 1,(BATCH_SIZE,100))\n",
        "    s = seed.shape\n",
        "    zero = np.zeros((s[0], 5))  \n",
        "    one = np.ones((s[0], 1)) \n",
        "    class1 = np.append(zero, one, axis=1)\n",
        "    return np.append(seed, class1, axis=1)\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    seed = batch_seed(BATCH_SIZE)\n",
        "    start = time.time()\n",
        "    gen_loss_total_list = []\n",
        "    disc_loss_total_list = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        gen_loss_list = []\n",
        "        disc_loss_list = []\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            t = train_step(image_batch,seed)\n",
        "            gen_loss_list.append(t[0])\n",
        "            disc_loss_list.append(t[1])\n",
        "\n",
        "        g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
        "        d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
        "\n",
        "        epoch_elapsed = time.time()-epoch_start\n",
        "        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss},'f' {hms_string(epoch_elapsed)}')\n",
        "\n",
        "        gen_loss_total_list.append(g_loss)\n",
        "        disc_loss_total_list.append(d_loss)\n",
        "        \n",
        "    elapsed = time.time()-start\n",
        "    print (f'Training time: {hms_string(elapsed)}')\n",
        "    return gen_loss_total_list , disc_loss_total_list\n",
        "\n",
        "def get_class_all(n,numofclass,betchsize):\n",
        "  if n == numofclass:\n",
        "    one = tf.zeros((betchsize,64,64,numofclass-1))\n",
        "    two = tf.ones((betchsize,64,64,1))\n",
        "    return tf.concat(one,two,3)\n",
        "  elif n == 1:\n",
        "    one = tf.ones((betchsize,64,64,1))\n",
        "    two = tf.zeros((betchsize,64,64,numofclass-1))\n",
        "    return tf.concat(one,two,3)\n",
        "  else:\n",
        "    one = tf.zeros((betchsize,64,64,n-1))\n",
        "    two = tf.ones((betchsize,64,64,1))\n",
        "    three = tf.zeros((betchsize,64,64,numofclass-n))\n",
        "  return tf.concat(one,two,three,3)"
      ],
      "metadata": {
        "id": "384K4rz8ZFhx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#arrange data for traing\n",
        "training_data_r = resizer(training_data_normalize, training=False)\n",
        "s = training_data_r.shape\n",
        "for training_data_by_class in list(training_data_r)\n",
        "  class1 = get_class_all(n,i%6,training_data_by_class.shape[0])\n",
        "  training_data_r1 = np.append(training_data_r, class1, axis=3)\n",
        "  training_data_r1[:, :, :, 2] = np.ones_like(training_data_r1[:, :, :, 2])\n",
        "print(training_data_r1.shape)"
      ],
      "metadata": {
        "id": "zt7Af9uCKW-q",
        "outputId": "fc9d54f4-c222-4423-b01d-351caaf49a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-92842c520885>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    for training_data_by_class in training_data_r\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_r = resizer(np.squeeze(training_data_normalize), training=False)\n",
        "print(training_data_r.shape)"
      ],
      "metadata": {
        "id": "O67lRTrcI4b4",
        "outputId": "8f0d0dd2-fa05-4c51-d78b-ed951b44a55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-66cc139ea13a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_normalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\u001b[0m\u001b[1;32m    264\u001b[0m                              \u001b[0;34m'incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                              \u001b[0;34mf'expected shape={spec.shape}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 100, 49, 3), found shape=(1, 939, 100, 49, 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "rZ1MwVb6ZjDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load data:\n",
        "print(\"Load training image binary...\")\n",
        "training_data =[];\n",
        "for action_file in tqdm(os.listdir(path_folder)):\n",
        "    training_binary_path = os.path.join(path_folder,action_file)\n",
        "    training_data.append(np.load(training_binary_path))\n",
        "\n",
        "#cast to np array\n",
        "training_data = np.asanyarray(training_data)\n",
        "\n",
        "#normlize\n",
        "print(\"Normlizing all....\")\n",
        "training_data_normalize =[];\n",
        "for i in tqdm(range(len(training_data))):\n",
        "  training_data[i][:] = tf.convert_to_tensor(training_data[i][:])\n",
        "  training_data_normalize.append(tf.map_fn(lambda x: tf.image.per_image_standardization(x), training_data, parallel_iterations=training_data.shape[0], dtype=tf.float32))\n",
        "\n",
        "#build resizer\n",
        "print(\"building resizer (64,64,100,49,3) ....\")\n",
        "resizer = build_image_resizer(64,64,100,49,3)\n",
        "\n",
        "\n",
        "#arrange data for traing\n",
        "training_data_r = resizer(training_data_normalize, training=False)\n",
        "s = training_data_r.shape\n",
        "for training_data_by_class in training_data_r\n",
        "  class1 = get_class_all(n,i%6,training_data_by_class.shape[0])\n",
        "  training_data_r1 = np.append(training_data_r, class1, axis=3)\n",
        "  training_data_r1[:, :, :, 2] = np.ones_like(training_data_r1[:, :, :, 2])\n",
        "print(training_data_r1.shape)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVOkXnfUZj0i",
        "outputId": "fcb82632-de1f-40cc-83f9-26d4111ce72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load training image binary...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 120/120 [00:01<00:00, 63.53it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data[0][:].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWmnW5LFcm7j",
        "outputId": "e88a614d-224e-4582-eb04-2d836891299b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(939, 100, 49, 3)\n"
          ]
        }
      ]
    }
  ]
}